{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xyNh2il1m9QQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class DQN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Perceptrón multicapa de 2 capas de 32 unidades y una capa de salida\n",
        "    \"\"\"\n",
        "    def __init__(self, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32)  # Sin activación\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Construcción de las capas\n",
        "        \"\"\"\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# Número de acciones (salida de la red)\n",
        "num_actions = 4  # Cambia esto según el problema\n",
        "\n",
        "# Crear las redes principal y objetivo\n",
        "main_nn = DQN(num_actions)  # Red principal\n",
        "target_nn = DQN(num_actions)  # Red objetivo\n",
        "\n",
        "# Optimizador y función de pérdida\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)  # Optimizador Adam\n",
        "mse = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W-xCvQmBnDYF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience replay buffer that samples uniformly.\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        idx = np.random.choice(len(self.buffer), num_samples)\n",
        "\n",
        "        for i in idx:\n",
        "            state, action, reward, next_state, done = self.buffer[i]\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HwdGtudHnFKr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Función para seleccionar una acción epsilon-greedy\n",
        "def select_epsilon_greedy_action(state, epsilon, env, main_nn):\n",
        "    \"\"\"\n",
        "    Selecciona una acción aleatoria con probabilidad epsilon,\n",
        "    o la mejor acción según la red principal con probabilidad 1 - epsilon.\n",
        "    \"\"\"\n",
        "    result = tf.random.uniform((1,))\n",
        "    if result < epsilon:\n",
        "        return env.action_space.sample()  # Acción aleatoria\n",
        "    else:\n",
        "        return tf.argmax(main_nn(state)[0]).numpy()  # Acción greedy\n",
        "\n",
        "# Función de entrenamiento con TensorFlow@tf.function para optimización\n",
        "discount = 0.99  # Factor de descuento (gamma)\n",
        "\n",
        "@tf.function\n",
        "def train_step(states, actions, rewards, next_states, dones, main_nn, target_nn, optimizer, mse, num_actions):\n",
        "    \"\"\"\n",
        "    Ejecuta un paso de entrenamiento de la red principal.\n",
        "    \"\"\"\n",
        "    # Calcular los valores Q para los siguientes estados con la red objetivo\n",
        "    next_qs = target_nn(next_states)\n",
        "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
        "\n",
        "    # Calcular las etiquetas objetivo para la función de pérdida\n",
        "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        qs = main_nn(states)\n",
        "        action_masks = tf.one_hot(actions, num_actions)\n",
        "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
        "        loss = mse(target, masked_qs)\n",
        "\n",
        "    # Calcular y aplicar gradientes\n",
        "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRWk_FaunmPj",
        "outputId": "3c77a762-2042-42ad-f348-3b3c2dcc3d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0/1000. Epsilon: 0.990. Reward in last 100 episodes: -118.519\n",
            "Episode 50/1000. Epsilon: 0.490. Reward in last 100 episodes: -93.285\n",
            "Episode 100/1000. Epsilon: 0.100. Reward in last 100 episodes: -90.993\n",
            "Episode 150/1000. Epsilon: 0.100. Reward in last 100 episodes: 22.597\n",
            "Episode 200/1000. Epsilon: 0.100. Reward in last 100 episodes: 115.000\n",
            "Episode 250/1000. Epsilon: 0.100. Reward in last 100 episodes: 118.040\n",
            "Episode 300/1000. Epsilon: 0.100. Reward in last 100 episodes: 105.967\n",
            "Episode 350/1000. Epsilon: 0.100. Reward in last 100 episodes: 88.349\n",
            "Episode 400/1000. Epsilon: 0.100. Reward in last 100 episodes: 126.601\n",
            "Episode 450/1000. Epsilon: 0.100. Reward in last 100 episodes: 155.706\n",
            "Episode 500/1000. Epsilon: 0.100. Reward in last 100 episodes: 161.419\n",
            "Episode 550/1000. Epsilon: 0.100. Reward in last 100 episodes: 171.899\n",
            "Episode 600/1000. Epsilon: 0.100. Reward in last 100 episodes: 190.788\n",
            "Episode 650/1000. Epsilon: 0.100. Reward in last 100 episodes: 185.801\n",
            "Episode 700/1000. Epsilon: 0.100. Reward in last 100 episodes: 195.702\n",
            "Episode 750/1000. Epsilon: 0.100. Reward in last 100 episodes: 225.403\n",
            "Episode 800/1000. Epsilon: 0.100. Reward in last 100 episodes: 237.387\n",
            "Episode 850/1000. Epsilon: 0.100. Reward in last 100 episodes: 230.412\n",
            "Episode 900/1000. Epsilon: 0.100. Reward in last 100 episodes: 224.399\n",
            "Episode 950/1000. Epsilon: 0.100. Reward in last 100 episodes: 224.192\n",
            "Episode 1000/1000. Epsilon: 0.100. Reward in last 100 episodes: 200.718\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gymnasium as gym\n",
        "\n",
        "# Hiperparámetros\n",
        "num_episodes = 1000\n",
        "epsilon = 1.0\n",
        "batch_size = 32\n",
        "discount = 0.99\n",
        "buffer = ReplayBuffer(100000)\n",
        "cur_frame = 0\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
        "\n",
        "# Seguimiento de recompensas recientes\n",
        "last_100_ep_rewards = []\n",
        "\n",
        "last_best_reward = -np.inf\n",
        "\n",
        "# Bucle principal de entrenamiento\n",
        "for episode in range(num_episodes + 1):\n",
        "    state, _ = env.reset()  # Reiniciar el entorno\n",
        "    ep_reward, done = 0, False\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "\n",
        "    while not done:\n",
        "        state_in = tf.convert_to_tensor([state], dtype=tf.float32)  # Asegura tensor de forma correcta\n",
        "        action = select_epsilon_greedy_action(state_in, epsilon, env, main_nn)\n",
        "        next_state, reward, done, _, _ = env.step(action)  # Algunos entornos devuelven más valores\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "        ep_reward += reward\n",
        "\n",
        "        # Añadir experiencia al buffer\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        cur_frame += 1\n",
        "\n",
        "        # Actualizar pesos de la red objetivo cada 2000 frames\n",
        "        if cur_frame % 2000 == 0:\n",
        "            target_nn.set_weights(main_nn.get_weights())\n",
        "\n",
        "        # Entrenar la red si hay suficientes muestras en el buffer\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "            loss = train_step(states, actions, rewards, next_states, dones, main_nn, target_nn, optimizer, mse, num_actions)\n",
        "\n",
        "    # Reducir epsilon para disminuir la exploración\n",
        "    if episode < 950:\n",
        "        epsilon = max(epsilon - 0.01, 0.1)\n",
        "\n",
        "    # Actualizar recompensas de los últimos 100 episodios\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards.pop(0)\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "    # Imprimir información de progreso cada 50 episodios\n",
        "    if episode % 50 == 0:\n",
        "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
        "              f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
        "    \n",
        "        if last_best_reward < np.mean(last_100_ep_rewards):\n",
        "            main_nn.save_weights(\"best_model_main.weights.h5\")\n",
        "            target_nn.save_weights(\"best_model_target.weights.h5\")\n",
        "            last_best_reward = np.mean(last_100_ep_rewards)\n",
        "\n",
        "\n",
        "# Cerrar el entorno\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_nn.save_weights('main_nn_weights_episode.weights.h5')\n",
        "target_nn.save_weights('target_nn_weights_episode.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipe7EpoQo7TE"
      },
      "outputs": [],
      "source": [
        "main_nn.load_weights('.h5')\n",
        "target_nn.load_weights('.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
