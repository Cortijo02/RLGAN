{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrantes de la prácticas\n",
        "\n",
        "* Alejandro Cortijo Benito\n",
        "* Alejandro García Mota"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Librerias utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Arquitectura del DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos tomado como referencia la red del `Lunar Lander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyNh2il1m9QQ"
      },
      "outputs": [],
      "source": [
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32) \n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# Número de acciones (salida de la red)\n",
        "num_actions = 4 \n",
        "\n",
        "main_nn = DQN(num_actions)  \n",
        "target_nn = DQN(num_actions)  \n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        idx = np.random.choice(len(self.buffer), num_samples)\n",
        "\n",
        "        for i in idx:\n",
        "            state, action, reward, next_state, done = self.buffer[i]\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Política "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En base a la experiencia de la primera parte de la práctica, decidimos usar una política greedy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_epsilon_greedy_action(state, epsilon, env, main_nn):\n",
        "    result = tf.random.uniform((1,))\n",
        "    if result < epsilon:\n",
        "        return env.action_space.sample()  \n",
        "    else:\n",
        "        return tf.argmax(main_nn(state)[0]).numpy() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usando como referencia el método de entrenamiento propuesto en las diapositivas, adaptamos el método para nuestro caso de estudio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwdGtudHnFKr"
      },
      "outputs": [],
      "source": [
        "discount = 0.99\n",
        "\n",
        "@tf.function\n",
        "def train_step(states, actions, rewards, next_states, dones, main_nn, target_nn, optimizer, mse, num_actions):\n",
        "    next_qs = target_nn(next_states)\n",
        "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
        "\n",
        "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        qs = main_nn(states)\n",
        "        action_masks = tf.one_hot(actions, num_actions)\n",
        "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
        "        loss = mse(target, masked_qs)\n",
        "\n",
        "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos los hiperparámetros de las diapositivas, intentamos cambiarlos para tratar de mejor los resultados pero no pudimos. Así que decidimos mantener los de las diapositivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hiperparámetros\n",
        "num_episodes = 1000\n",
        "epsilon = 1.0\n",
        "batch_size = 32\n",
        "discount = 0.99\n",
        "buffer = ReplayBuffer(100000)\n",
        "cur_frame = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usando las funciones definidas previamente, empezamos el aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRWk_FaunmPj",
        "outputId": "3c77a762-2042-42ad-f348-3b3c2dcc3d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0/1000. Epsilon: 0.990. Reward in last 100 episodes: -118.519\n",
            "Episode 50/1000. Epsilon: 0.490. Reward in last 100 episodes: -93.285\n",
            "Episode 100/1000. Epsilon: 0.100. Reward in last 100 episodes: -90.993\n",
            "Episode 150/1000. Epsilon: 0.100. Reward in last 100 episodes: 22.597\n",
            "Episode 200/1000. Epsilon: 0.100. Reward in last 100 episodes: 115.000\n",
            "Episode 250/1000. Epsilon: 0.100. Reward in last 100 episodes: 118.040\n",
            "Episode 300/1000. Epsilon: 0.100. Reward in last 100 episodes: 105.967\n",
            "Episode 350/1000. Epsilon: 0.100. Reward in last 100 episodes: 88.349\n",
            "Episode 400/1000. Epsilon: 0.100. Reward in last 100 episodes: 126.601\n",
            "Episode 450/1000. Epsilon: 0.100. Reward in last 100 episodes: 155.706\n",
            "Episode 500/1000. Epsilon: 0.100. Reward in last 100 episodes: 161.419\n",
            "Episode 550/1000. Epsilon: 0.100. Reward in last 100 episodes: 171.899\n",
            "Episode 600/1000. Epsilon: 0.100. Reward in last 100 episodes: 190.788\n",
            "Episode 650/1000. Epsilon: 0.100. Reward in last 100 episodes: 185.801\n",
            "Episode 700/1000. Epsilon: 0.100. Reward in last 100 episodes: 195.702\n",
            "Episode 750/1000. Epsilon: 0.100. Reward in last 100 episodes: 225.403\n",
            "Episode 800/1000. Epsilon: 0.100. Reward in last 100 episodes: 237.387\n",
            "Episode 850/1000. Epsilon: 0.100. Reward in last 100 episodes: 230.412\n",
            "Episode 900/1000. Epsilon: 0.100. Reward in last 100 episodes: 224.399\n",
            "Episode 950/1000. Epsilon: 0.100. Reward in last 100 episodes: 224.192\n",
            "Episode 1000/1000. Epsilon: 0.100. Reward in last 100 episodes: 200.718\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
        "\n",
        "last_100_ep_rewards = []\n",
        "\n",
        "last_best_reward = -np.inf\n",
        "\n",
        "for episode in range(num_episodes + 1):\n",
        "    state, _ = env.reset()  \n",
        "    ep_reward, done = 0, False\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "\n",
        "    while not done:\n",
        "        state_in = tf.convert_to_tensor([state], dtype=tf.float32) \n",
        "        action = select_epsilon_greedy_action(state_in, epsilon, env, main_nn)\n",
        "        next_state, reward, done, _, _ = env.step(action)  \n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "        ep_reward += reward\n",
        "\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        cur_frame += 1\n",
        "\n",
        "        if cur_frame % 2000 == 0:\n",
        "            target_nn.set_weights(main_nn.get_weights())\n",
        "\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "            loss = train_step(states, actions, rewards, next_states, dones, main_nn, target_nn, optimizer, mse, num_actions)\n",
        "\n",
        "    if episode < 950:\n",
        "        epsilon = max(epsilon - 0.01, 0.1)\n",
        "\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards.pop(0)\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "    if episode % 50 == 0:\n",
        "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
        "              f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
        "    \n",
        "        if last_best_reward < np.mean(last_100_ep_rewards):\n",
        "            main_nn.save_weights(\"best_model_main.weights.h5\")\n",
        "            target_nn.save_weights(\"best_model_target.weights.h5\")\n",
        "            last_best_reward = np.mean(last_100_ep_rewards)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluación de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos los mejores pesos, ejecutamos `5` experimentos y nos guardamos los resultados en un `.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved at lunar_lander_results.mp4\n"
          ]
        }
      ],
      "source": [
        "# Inferencia y generación de video\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "main_nn.load_weights(\"best_model_main.weights.h5\")\n",
        "\n",
        "frames = []\n",
        "for episode in range(5):  \n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        state_in = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        action = tf.argmax(main_nn(state_in)[0]).numpy()\n",
        "        next_state, _, done, _, _ = env.step(action)\n",
        "        frames.append(env.render())\n",
        "        state = next_state\n",
        "\n",
        "# Guardar el video\n",
        "video_path = \"lunar_lander_results.mp4\"\n",
        "imageio.mimsave(video_path, frames, fps=30, codec='libx264')\n",
        "print(f\"Video saved at {video_path}\")\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
